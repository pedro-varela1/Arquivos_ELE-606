{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWQKSj0Es9LSGlua8eq1P6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedro-varela1/Arquivos_ELE-606/blob/main/Apresetacao_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seminário e Apresentações - Machine Learning with Python\n",
        "## ELE606 - Tópicos Especiais em Inteligência Artificial\n",
        "\n",
        "> Aluno: [Pedro Artur Varela](https://github.com/pedro-varela1)\n",
        "  \n",
        "![divider](https://webstockreview.net/images/divider-clipart-design-line-5.png)"
      ],
      "metadata": {
        "id": "54f7-NQufTj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cap 3 - Métodos para Machine Learning\n",
        "\n",
        "Os métodos de Machine Learning são ferramentas essenciais para ensinar algoritmos a aprender com dados, sem uma programação explícita. Esses métodos se dividem principalmente em três categorias: Aprendizado Supervisionado, Aprendizado Não Supervisionado e Aprendizado por Reforço.\n",
        "\n",
        "No Aprendizado Supervisionado, os algoritmos são treinados com dados rotulados, permitindo que aprendam a associar entradas a respostas conhecidas. Isso é útil em tarefas como previsão de valores ou classificação de dados em categorias específicas.\n",
        "\n",
        "Já no Aprendizado Não Supervisionado, não há rótulos nos dados. Aqui, os algoritmos buscam padrões, estruturas ou relações intrínsecas nos dados, sendo úteis para agrupamento, redução de dimensionalidade e descoberta de associações entre variáveis.\n",
        "\n",
        "Por fim, o Aprendizado por Reforço envolve agentes que aprendem a tomar ações em um ambiente para maximizar recompensas. Esses algoritmos recebem feedback em forma de recompensa ou punição, aprendendo a tomar decisões sequenciais em ambientes complexos.\n",
        "\n",
        "A escolha do método de Machine Learning depende da disponibilidade e natureza dos dados, assim como da tarefa a ser executada. Se houver dados rotulados e a tarefa for prever ou classificar, o aprendizado supervisionado é adequado. Se não houver rótulos e a meta for encontrar estruturas nos dados, o aprendizado não supervisionado é mais apropriado. Já o aprendizado por reforço é utilizado para treinar agentes em ambientes onde decisões sequenciais são necessárias para maximizar recompensas."
      ],
      "metadata": {
        "id": "Furk5kg4fqJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cap 5 - Entendendo Dados com Estatística\n",
        "\n",
        "A compreensão dos dados por meio da estatística é fundamental para extrair insights significativos e tomar decisões informadas. A análise estatística começa com a exploração do \"raw data\" (dados brutos), onde uma série de etapas é realizada para compreender a estrutura e natureza dos dados.\n",
        "\n",
        "Inicialmente, a análise envolve a verificação das dimensões do conjunto de dados, ou seja, quantas entradas (amostras) e atributos (variáveis) estão presentes. Compreender a quantidade de dados disponíveis é crucial para determinar a abrangência e a confiabilidade das análises subsequentes.\n",
        "\n",
        "Em seguida, é importante examinar cada atributo individualmente, compreendendo suas características. Isso inclui resumos estatísticos básicos, como média, mediana, moda, desvio padrão e quartis, fornecendo uma visão geral da distribuição dos valores em cada atributo. Isso ajuda a identificar outliers (valores atípicos) que podem distorcer análises posteriores.\n",
        "\n",
        "A compreensão da distribuição de cada atributo é essencial para entender a sua variabilidade e centralidade. Gráficos, como histogramas, boxplots e gráficos de dispersão, são úteis para visualizar essa distribuição e identificar padrões, tendências e possíveis relações entre variáveis.\n",
        "\n",
        "Além disso, a correlação entre atributos é explorada para entender como eles se relacionam entre si. A análise da correlação ajuda a identificar quais atributos estão mais fortemente relacionados e pode sugerir insights valiosos para modelos preditivos ou explicativos.\n",
        "\n",
        "Por fim, a correção de cada atributo, quando necessário, pode envolver várias técnicas, como preenchimento de valores ausentes, normalização para escala comum, transformações para adequação à distribuição desejada, entre outras técnicas estatísticas. Essas correções visam preparar os dados para uma análise mais precisa e para garantir que os modelos de Machine Learning ou estatísticos sejam robustos e eficazes."
      ],
      "metadata": {
        "id": "QD4t4usWg-kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cap 7 - Preparando os Dados\n",
        "\n",
        "A preparação de dados desempenha um papel crucial na garantia da qualidade e na eficácia das análises e modelos de Machine Learning. Quatro técnicas comuns nesse processo são a normalização, binarização, padronização e labeling.\n",
        "\n",
        "A **normalização** é utilizada para escalar os valores dos atributos para uma faixa específica, como por exemplo, entre 0 e 1 ou -1 e 1. Isso é particularmente útil quando diferentes atributos têm escalas variadas, garantindo que todos contribuam igualmente para a análise.\n",
        "\n",
        "Já a **binarização** é aplicada para transformar variáveis numéricas em variáveis binárias, por exemplo, convertendo valores acima de um limiar específico para 1 e abaixo desse limiar para 0. Essa técnica é útil em cenários onde a presença ou ausência de um atributo é mais relevante do que seu valor específico.\n",
        "\n",
        "A **padronização** é similar à normalização, mas ao invés de ajustar os valores para uma faixa específica, transforma os dados de maneira que a média seja 0 e o desvio padrão seja 1. Isso é valioso em algoritmos sensíveis à escala dos atributos, como aqueles baseados em distância, por exemplo, muitos algoritmos de clustering.\n",
        "\n",
        "Por fim, o **labeling** é utilizado para converter variáveis categóricas em números ou valores discretos. Isso é fundamental para permitir que algoritmos de Machine Learning entendam e processem essas variáveis, já que muitos deles trabalham melhor com dados numéricos. Um exemplo é converter categorias como \"alto\", \"médio\" e \"baixo\" em valores numéricos como 2, 1 e 0, respectivamente."
      ],
      "metadata": {
        "id": "kiCVUCIhhtau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cap 9 - Introdução à Classificação\n",
        "\n",
        "Os algoritmos de classificação são fundamentais no campo da Machine Learning para prever a categoria à qual um novo ponto de dados pertence. Existem diversos tipos de algoritmos de classificação, como K-Nearest Neighbors (KNN), Árvores de Decisão, Naive Bayes, Support Vector Machines (SVM), e Redes Neurais, entre outros. Cada um desses algoritmos tem suas próprias características, vantagens e limitações em diferentes cenários.\n",
        "\n",
        "Em Python, bibliotecas como scikit-learn e TensorFlow oferecem uma ampla gama de algoritmos de classificação prontos para uso. Por exemplo, com o scikit-learn, é possível implementar esses algoritmos em poucas linhas de código, facilitando a construção e avaliação de modelos de classificação.\n",
        "\n",
        "Ao avaliar a eficácia de um modelo de classificação, várias métricas de validação são consideradas. Entre as principais métricas estão a precisão (accuracy), que calcula a proporção de previsões corretas em relação ao total de previsões, a recall (revocação), que mede a proporção de verdadeiros positivos em relação a todos os valores verdadeiros da classe, e a F1-score, que é uma média harmônica entre precisão e recall.\n",
        "\n",
        "A matriz de confusão é uma ferramenta importante para visualizar o desempenho de um algoritmo de classificação. Ela mostra a relação entre as previsões do modelo e os valores reais, dividindo as previsões em quatro categorias: verdadeiros positivos (TP), falsos positivos (FP), verdadeiros negativos (TN) e falsos negativos (FN). Essa matriz é útil para entender onde o modelo está acertando e onde está errando.\n",
        "\n",
        "**Para expor esses conceitos, serão abordados os algoritmos: _SVM_ (Cap 11), _Naive Bayes_ (Cap 13) e o _kNN_ (Cap 21).**"
      ],
      "metadata": {
        "id": "YWkF-2PijDIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cap 15 - Técnicas de Regressão\n",
        "\n",
        "A regressão é uma técnica fundamental em Machine Learning usada para prever valores numéricos com base em dados históricos. Ela estabelece uma relação entre variáveis independentes e dependentes, permitindo estimar ou prever um resultado contínuo. Existem diferentes tipos de algoritmos de regressão adequados para diferentes contextos.\n",
        "\n",
        "A regressão linear é o tipo mais simples e comum, onde se estabelece uma relação linear entre a variável dependente e uma ou mais variáveis independentes. Por exemplo, pode ser usada para prever o preço de uma casa com base em características como tamanho, número de quartos, etc.\n",
        "\n",
        "A regressão polinomial é uma extensão da regressão linear, permitindo ajustar uma curva polinomial aos dados. É útil quando os dados seguem um padrão não linear, podendo capturar melhor relações complexas entre variáveis.\n",
        "\n",
        "Outros tipos de regressão incluem a regressão logística, que é usada para problemas de classificação binária, a regressão Ridge e Lasso, que são variações da regressão linear com penalizações para evitar overfitting, e a regressão de séries temporais, usada para prever valores futuros com base em dados temporais históricos."
      ],
      "metadata": {
        "id": "nRlhz7jFj6xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cap 17 - Introdução aos Algoritmos de Clustering\n",
        "\n",
        "Os algoritmos de clustering são técnicas de aprendizado não supervisionado que agrupam dados similares em conjuntos ou clusters. Esses algoritmos exploram a estrutura dos dados, identificando padrões e relações entre os pontos de dados sem a necessidade de rótulos prévios.\n",
        "\n",
        "Antes de aplicar algoritmos de clustering, é essencial formatar os dados de maneira apropriada, garantindo que sejam numéricos e normalizados, se necessário. Isso assegura que os algoritmos funcionem corretamente, considerando a similaridade entre os atributos.\n",
        "\n",
        "A medição de performance em algoritmos de clustering é desafiadora, já que não existe uma verdade absoluta sobre os agrupamentos. Uma métrica comum é o índice silhouette, que avalia a coesão dos clusters e a separação entre eles. Esse índice varia de -1 a 1, onde valores próximos de 1 indicam clusters bem separados, valores próximos de 0 indicam sobreposição entre clusters e valores negativos indicam que os pontos podem estar no cluster errado.\n",
        "\n",
        "Existem vários tipos de algoritmos de clustering. O K-Means é um dos mais populares, onde os dados são agrupados em K clusters, sendo K um número determinado pelo usuário. Outro algoritmo é o Mean Shift, uma técnica de clustering não paramétrico que não requer a definição prévia do número de clusters, diferenciando-se, nesse aspecto, de algoritmos como o K-Means.\n",
        "\n",
        "Uma vantagem do Mean Shift é a sua capacidade de identificar clusters de formas e tamanhos irregulares, adaptando-se dinamicamente à densidade dos dados. Isso torna o algoritmo robusto e capaz de encontrar clusters de diferentes densidades e formas geométricas.\n",
        "\n",
        "Para medir a performance do Mean Shift ou de qualquer algoritmo de clustering, o índice silhouette ainda pode ser aplicado. Ele avalia a coesão e a separação dos clusters, oferecendo uma métrica para compreender a qualidade dos agrupamentos produzidos pelo algoritmo.\n",
        "\n",
        "**Para expor esses conceitos, será abordado o algoritmo: _Mean Shift_ (Cap 19).**"
      ],
      "metadata": {
        "id": "MnN7rbRUlnib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cap 23 e 25 - Workflows Automáticos e Tuning um Modelo de Machine Learing\n",
        "\n",
        "A construção de workflows automatizados é crucial para otimizar o processo de Machine Learning. Isso envolve a criação de pipelines, que são sequências de etapas automatizadas, desde a preparação dos dados até a avaliação do modelo. Os pipelines garantem consistência e replicabilidade, permitindo uma fácil reprodução de experimentos e um desenvolvimento mais eficiente de modelos.\n",
        "\n",
        "A modelagem de pipelines em Machine Learning inclui diferentes estágios, como pré-processamento de dados, seleção de features, aplicação de algoritmos de Machine Learning e avaliação de desempenho. Estruturar esses estágios em pipelines ajuda na organização e na execução sequencial de tarefas, facilitando a manutenção e a escalabilidade do sistema.\n",
        "\n",
        "Além disso, estratégias para aumento de performance do modelo são fundamentais. Isso pode incluir a busca por hiperparâmetros otimizados, utilizando técnicas como Grid Search ou Random Search, que exploram diferentes combinações de parâmetros do modelo. A validação cruzada (cross-validation) também é uma estratégia essencial para garantir que o modelo generalize bem para dados não vistos durante o treinamento.\n",
        "\n",
        "Outras estratégias para melhorar a performance do modelo envolvem o balanceamento de classes em conjuntos de dados desbalanceados, a seleção cuidadosa de features relevantes, o uso de técnicas de regularização para evitar overfitting e a aplicação de técnicas de ensemble, como Bagging, Boosting e Stacking, que combinam múltiplos modelos para melhorar a precisão geral."
      ],
      "metadata": {
        "id": "bIpfZbiPnQuz"
      }
    }
  ]
}